# Related Work of Training Algorithm Optimization

This document briefly summarizes related work for neural network training acceleration. The works mainly comes from two aspects: hardware design and training algorithm.

## Hardware Accelerators

### [LNPU: A 25.3TFLOPS/W Sparse Deep-Neural-Network Learning Processor with Fine-Grained Mixed Precision of FP8-FP16](https://ieeexplore.ieee.org/abstract/document/8662302/)

### [A Framework for Acceleration of CNN Training on Deeply-Pipelined FPGA Clusters with Workload Balancing](https://ieeexplore.ieee.org/abstract/document/8533530/)

### [Long Live TIME: Improving Lifetime for Training-In-Memory Engines by Structured Gradient Sparsification](https://nicsefc.ee.tsinghua.edu.cn/media/publications/2018/DAC18_251.pdf)

### [Training Low Bitwidth Convolutional Neural Network on RRAM](http://nicsefc.ee.tsinghua.edu.cn/media/publications/2018/ASPDAC18_239.pdf)

### [Gist: Efficient Data Encoding for Deep Neural Network Training](http://www.cs.toronto.edu/~pekhimenko/Papers/ISCA18-Gist.pdf)

### [Compressing DMA Engine: Leveraging Activation Sparsity for Training Deep Neural Networks](https://arxiv.org/pdf/1705.01626)

### [A Network-Centric Hardware/Algorithm Co-Design to Accelerate Distributed Training of Deep Neural Networks](https://www.cc.gatech.edu/~hadi/doc/paper/2018-micro-inceptionn.pdf)

### [Processing-in-Memory for Energy-efficient Neural Network Training: A Heterogeneous Approach](http://cseweb.ucsd.edu/~jzhao/files/pim-micro2018.pdf)

### [ClosNets: Batchless DNN Training with On-Chip A Priori Sparse Neural Topologies](http://kalman.mee.tcd.ie/fpl2018/content/pdfs/FPL2018-43iDzVTplcpussvbfIaaHz/7mIDkpe7PISkDCNFSwdRS0/7d5I8bSDNRGufI7K6e55ek.pdf)

## Training Algorithms

### [Retraining-Based Iterative Weight Quantization for Deep Neural Networks](https://arxiv.org/pdf/1805.11233)

### [A Survey on Methods and Theories of Quantized Neural Networks](https://arxiv.org/pdf/1808.04752)

### [Learning to Quantize Deep Networks by Optimizing Quantization Intervals with Task Loss](https://arxiv.org/pdf/1808.05779)

### [Accumulation Bit-Width Scaling For Ultra-Low Precision Training Oof Deep Networks](https://arxiv.org/pdf/1901.06588)

### [Scalable Methods for 8-bit Training of Neural Networks](http://papers.nips.cc/paper/7761-scalable-methods-for-8-bit-training-of-neural-networks.pdf)

### [Training Deep Neural Networks with 8-bit Floating Point Numbers](https://papers.nips.cc/paper/7994-training-deep-neural-networks-with-8-bit-floating-point-numbers.pdf)

### [Analysis of Quantized Models](https://openreview.net/pdf?id=ryM_IoAqYX)


